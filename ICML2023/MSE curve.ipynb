{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699df5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32c495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import winsound as ws\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random, os, winsound\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from numpy.linalg import slogdet, inv\n",
    "from scipy import optimize\n",
    "from scipy.linalg import block_diag\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402606a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "class config:\n",
    "    seed = 42\n",
    "    device = \"cuda:0\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_model(X):\n",
    "    # make fixed part of the mean model (=marginal mean)\n",
    "    s = np.sum(X, axis=1)\n",
    "    mu = s * np.cos(s) + 2*X[:,0]*X[:,1]\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mean_model():\n",
    "\n",
    "    input_X  = Input(shape=(np.shape(X_train)[1],),  dtype='float32')\n",
    "    input_Z1 = Input(shape=(np.shape(Z1_train)[1],), dtype='float32')\n",
    "    input_Z2 = Input(shape=(np.shape(Z2_train)[1],), dtype='float32')\n",
    "    \n",
    "    m   = Dense(100, activation='relu')(input_X)\n",
    "    m   = Dense(50, activation='relu')(m)\n",
    "    m   = Dense(25, activation='relu')(m)\n",
    "    m   = Dense(12, activation='relu')(m)\n",
    "    \n",
    "    xb  = Dense(1, activation='linear')(m)\n",
    "    zv1 = Dense(1, activation='linear', use_bias=False)(input_Z1)\n",
    "    zv2 = Dense(1, activation='linear', use_bias=False)(input_Z2)\n",
    "    # zv1 = Dense(1, activation='linear', use_bias=False, kernel_initializer=initializers.Zeros())(input_Z1)\n",
    "    # zv2 = Dense(1, activation='linear', use_bias=False, kernel_initializer=initializers.Zeros())(input_Z2)\n",
    "\n",
    "    mean_model = Model(inputs=[input_X, input_Z1, input_Z2], outputs=[xb, zv1, zv2])\n",
    "    return mean_model\n",
    "\n",
    "def make_marginal_model():\n",
    "\n",
    "    input_X  = Input(shape=(np.shape(X_train)[1],),  dtype='float32')\n",
    "    input_Z1 = Input(shape=(np.shape(Z1_train)[1],), dtype='float32')\n",
    "    input_Z2 = Input(shape=(np.shape(Z2_train)[1],), dtype='float32')\n",
    "    \n",
    "    m   = Dense(100, activation='relu')(input_X)\n",
    "    m   = Dense(50, activation='relu')(m)\n",
    "    m   = Dense(25, activation='relu')(m)\n",
    "    m   = Dense(12, activation='relu')(m)\n",
    "    \n",
    "    xb  = Dense(1, activation='linear')(m)\n",
    "    \n",
    "    mean_model = Model(inputs=[input_X, input_Z1, input_Z2], outputs=[xb])\n",
    "    return mean_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0d4c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hlik_loss_mean(mean_model, mean_inputs, y, phi, lam1, lam2, batch_ratio=1.):    \n",
    "    # batch_ratio = batch_size / N_train (= 1 for full batch or validation)\n",
    "    \n",
    "    mu = K.transpose(K.sum(mean_model(mean_inputs), axis=0))\n",
    "    loss = K.sum(K.square(y - mu)) / phi / np.shape(y)[0]\n",
    "    \n",
    "    weights1 = mean_model.weights[-1]\n",
    "    weights2= mean_model.weights[-2]\n",
    "    loss += (K.sum(K.square(weights1))/lam1 + K.sum(K.square(weights2))/lam2) * batch_ratio / np.shape(y)[0]\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def update_mean_params(mean_model, mean_inputs, y, loss_ftn, optimizer, phi, lam1, lam2, batch_ratio=1.):  \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_ftn(mean_model, mean_inputs, y, phi, lam1, lam2, batch_ratio)        \n",
    "    gradients = tape.gradient(loss, mean_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, mean_model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "def train_mean_one_epoch(mean_model, train_batch, loss_ftn, optimizer, phi, lam1, lam2, batch_ratio=1.):    \n",
    "    losses = [] \n",
    "    for step, (X_batch, Z1_batch, Z2_batch, y_batch) in enumerate(train_batch):        \n",
    "        loss = update_mean_params(mean_model, [X_batch, Z1_batch, Z2_batch], y_batch, loss_ftn, optimizer, phi, lam1, lam2, batch_ratio)\n",
    "        losses.append(loss)      \n",
    "    return losses\n",
    "\n",
    "def hlik_loss(mean_model, mean_inputs, y, phi, lam1, lam2, batch_ratio=1.):    \n",
    "    # batch_ratio = batch_size / N_train (= 1 for full batch or validation)\n",
    "    \n",
    "    mu = K.transpose(K.sum(mean_model(mean_inputs), axis=0))\n",
    "    loss = K.sum(K.square(y - mu)) / phi / np.shape(y)[0]\n",
    "    \n",
    "    weights1 = mean_model.weights[-1]\n",
    "    weights2= mean_model.weights[-2]\n",
    "    loss += (K.sum(K.square(weights1))/lam1 + K.sum(K.square(weights2))/lam2) * batch_ratio / np.shape(y)[0]\n",
    "    \n",
    "    ZZ = np.float32(np.concatenate(([mean_inputs[1], mean_inputs[2]]), axis=1))\n",
    "    DD = np.diag(np.repeat([lam1, lam2],q))\n",
    "    loss += (tf.linalg.slogdet(K.transpose(ZZ)@ZZ@DD+phi*tf.eye(Q))[1]+(np.shape(y)[0]-2*q)*K.log(phi)) / np.shape(y)[0]\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def update_hlik(mean_model, mean_inputs, y, loss_ftn, optimizer, phi, lam1, lam2, batch_ratio=1.):  \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_ftn(mean_model, mean_inputs, y, phi, lam1, lam2, batch_ratio)        \n",
    "    gradients = tape.gradient(loss, (mean_model.trainable_weights + [phi, lam1, lam2]))\n",
    "    optimizer.apply_gradients(zip(gradients, (mean_model.trainable_weights + [phi, lam1, lam2])))\n",
    "    return loss\n",
    "\n",
    "def train_hlik_one_epoch(mean_model, train_batch, loss_ftn, optimizer, phi, lam1, lam2, batch_ratio=1.):    \n",
    "    losses = [] \n",
    "    for step, (X_batch, Z1_batch, Z2_batch, y_batch) in enumerate(train_batch):        \n",
    "        loss = update_hlik(mean_model, [X_batch, Z1_batch, Z2_batch], y_batch, loss_ftn, optimizer, phi, lam1, lam2, batch_ratio)\n",
    "        losses.append(loss)      \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3edd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_disp_mle(ehat, v1, v2, phi, lam1, lam2, ZTZ):\n",
    "    \n",
    "    a0, a1, a2 = np.log(phi), np.log(lam1), np.log(lam2)\n",
    "    D1 = np.diag(np.repeat([np.exp(a1), 0], q))\n",
    "    D2 = np.diag(np.repeat([0, np.exp(a2)], q))\n",
    "    \n",
    "    Ainv = inv(ZTZ@np.diag(np.repeat([np.exp(a1), np.exp(a2)],q)) + np.eye(Q)*np.exp(a0))\n",
    "    A1, A2 = Ainv@ZTZ@D1, Ainv@ZTZ@D2\n",
    "    \n",
    "    ete, vtv1, vtv2 = np.sum(ehat**2), np.sum(v1**2), np.sum(v2**2)\n",
    "    \n",
    "    ddetda0 = np.exp(a0)*np.trace(Ainv)\n",
    "    ddetda1, ddetda2 = np.trace(A1), np.trace(A2)\n",
    "    \n",
    "    grad0 = -N +Q + np.exp(-a0)*ete + ddetda0\n",
    "    grad1 = np.exp(-a1)*vtv1 + ddetda1\n",
    "    grad2 = np.exp(-a2)*vtv2 + ddetda2\n",
    "    \n",
    "    hess0 = - np.exp(-a0)*ete  + ddetda0 - np.exp(2*a0)*np.trace(Ainv@Ainv)\n",
    "    hess1 = - np.exp(-a1)*vtv1 + ddetda1 - np.trace(A1@A1)\n",
    "    hess2 = - np.exp(-a2)*vtv2 + ddetda2 - np.trace(A2@A2)\n",
    "    \n",
    "    a0 += - grad0/hess0\n",
    "    a1 += - grad1/hess1\n",
    "    a2 += - grad2/hess2\n",
    "    \n",
    "    diff = np.abs(grad0/hess0+grad1/hess1+grad2/hess2)\n",
    "    \n",
    "    return(np.exp(a0), np.exp(a1), np.exp(a2), diff)\n",
    "\n",
    "def find_disp_mle(y, mu, v1, v2, phi_init, lam1_init, lam2_init, ZTZ, tolorence=1e-3, max_iter=100):\n",
    "    \n",
    "    ehat = y - mu\n",
    "    phi, lam1, lam2 = phi_init, lam1_init, lam2_init\n",
    "    for iteration in range(max_iter):\n",
    "        phi, lam1, lam2, diff = update_disp_mle(ehat, v1, v2, phi, lam1, lam2, ZTZ)\n",
    "        if np.abs(diff)<tolorence:\n",
    "            break\n",
    "    return(phi, lam1, lam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a27511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_loss(mean_model, mean_inputs, y, phi, lam1, lam2):    \n",
    "    # batch_ratio = batch_size / N_train (= 1 for full batch or validation)\n",
    "    \n",
    "    fX = K.transpose(mean_model(mean_inputs))\n",
    "    ZZ = np.float32(np.concatenate(([mean_inputs[1], mean_inputs[2]]), axis=1))\n",
    "    DD = (lam1*np.diag(np.repeat([1,0],q)) + lam2*np.diag(np.repeat([0,1],q)))\n",
    "    \n",
    "    loss = K.sum(K.square(y - fX)) / phi / np.shape(y)[0]    \n",
    "    loss -= ((y-fX)@ZZ@(lam1*np.diag(np.repeat([1,0],q)) + lam2*np.diag(np.repeat([0,1],q)))@tf.linalg.inv(K.transpose(ZZ)@ZZ@(lam1*np.diag(np.repeat([1,0],q)) + lam2*np.diag(np.repeat([0,1],q)))+phi*tf.eye(Q))@K.transpose(ZZ)@K.transpose(y-fX)/phi)[0,0] / np.shape(y)[0]\n",
    "    loss += (tf.linalg.slogdet(K.transpose(ZZ)@ZZ@DD+phi*tf.eye(Q))[1]+(np.shape(y)[0]-2*q)*K.log(phi)) / np.shape(y)[0]\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def update_marginal(mean_model, mean_inputs, y, phi, lam1, lam2, loss_ftn, optimizer):  \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_ftn(mean_model, mean_inputs, y, phi, lam1, lam2)\n",
    "    gradients = tape.gradient(loss, (mean_model.trainable_weights + [phi, lam1, lam2]))\n",
    "    optimizer.apply_gradients(zip(gradients, (mean_model.trainable_weights + [phi, lam1, lam2])))\n",
    "    return loss\n",
    "\n",
    "def train_marginal_one_epoch(mean_model, train_batch, phi, lam1, lam2, loss_ftn, optimizer):    \n",
    "    losses = [] \n",
    "    for step, (X_batch, Z1_batch, Z2_batch, y_batch) in enumerate(train_batch):        \n",
    "        loss = update_marginal(mean_model, [X_batch, Z1_batch, Z2_batch], y_batch,  phi, lam1, lam2, loss_ftn, optimizer)\n",
    "        losses.append(loss)      \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e84f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simul = 20\n",
    "N = 10000\n",
    "K_num = 2\n",
    "p, q = 10, 100\n",
    "Q = q*K_num\n",
    "lam, sig2 = .5, .5\n",
    "phi_init, lam1_init, lam2_init = 1., 1., 1.\n",
    "\n",
    "batch_size = 1024\n",
    "max_epochs = 100\n",
    "patience = 5\n",
    "\n",
    "mean_lr = 0.002\n",
    "mean_optimizer = Adam(learning_rate = mean_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9918f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_marginal = np.zeros((n_simul, max_epochs))\n",
    "time_hlik = np.zeros((n_simul, max_epochs))\n",
    "\n",
    "train_mse_marginal = np.zeros((n_simul, max_epochs))\n",
    "valid_mse_marginal = np.zeros((n_simul, max_epochs))\n",
    "\n",
    "train_mse_hlik = np.zeros((n_simul, max_epochs))\n",
    "valid_mse_hlik = np.zeros((n_simul, max_epochs))\n",
    "\n",
    "phi_marginal = np.zeros((n_simul, max_epochs))\n",
    "lam1_marginal = np.zeros((n_simul, max_epochs))\n",
    "lam2_marginal = np.zeros((n_simul, max_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b645a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repeat in range(n_simul):\n",
    "    \n",
    "    K.clear_session()\n",
    "    seed_everything(repeat)\n",
    "    \n",
    "    D = [ lam*np.identity(q) for k in range(K_num)]\n",
    "    v = [ np.random.multivariate_normal(np.zeros(q), D[k], 1)[0] for k in range(K_num) ]\n",
    "    z = [ np.random.choice(range(q), size=N, replace=True) for k in range(K_num) ]\n",
    "    epsilon = np.random.normal(0, np.sqrt(sig2), N)\n",
    "\n",
    "    X = np.random.uniform(-1, 1, (N,p))\n",
    "    fX = mean_model(X)\n",
    "\n",
    "    Z = [ pd.get_dummies(z[k]) for k in range(K_num)]\n",
    "    Z1 = np.float32(Z[0])\n",
    "    Z2 = np.float32(Z[1])\n",
    "\n",
    "    y = np.float32(fX + epsilon + sum([Z[k]@v[k] for k in range(K_num)]) )\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid, Z1_train, Z1_valid, Z2_train, Z2_valid = train_test_split(\n",
    "        X, y, Z1, Z2, test_size=0.2, random_state=42)\n",
    "\n",
    "    N_train, N_valid = np.shape(y_train)[0], np.shape(y_valid)[0]\n",
    "    batch_ratio = batch_size/N_train\n",
    "    marginal_batch = tf.data.Dataset.from_tensor_slices((X_train, Z1_train, Z2_train, y_train)).shuffle(N_train).batch(batch_size)\n",
    "    hlik_batch = tf.data.Dataset.from_tensor_slices((X_train, Z1_train, Z2_train, y_train)).shuffle(N_train).batch(batch_size)\n",
    "\n",
    "    # make marginal model\n",
    "    M = make_marginal_model()\n",
    "    phi  = tf.Variable(phi_init,  name='phi',  trainable=True, constraint=lambda x: tf.clip_by_value(x, 1e-18, np.infty))\n",
    "    lam1 = tf.Variable(lam1_init, name='lam1', trainable=True, constraint=lambda x: tf.clip_by_value(x, 1e-18, np.infty))\n",
    "    lam2 = tf.Variable(lam2_init, name='lam2', trainable=True, constraint=lambda x: tf.clip_by_value(x, 1e-18, np.infty))\n",
    "    \n",
    "    patience_marginal = 0\n",
    "    min_marginal_val_loss = np.infty\n",
    "    temp_start = time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        if epoch!=0:        \n",
    "            marginal_train_loss = train_marginal_one_epoch(M, marginal_batch, phi, lam1, lam2, marginal_loss, mean_optimizer)\n",
    "            marginal_val_loss = marginal_loss(M, [X_valid, Z1_valid, Z2_valid], y_valid, phi, lam1, lam2)\n",
    "          \n",
    "        fX_train, fX_valid = M([X_train, Z1_train, Z2_train]), M([X_valid, Z1_valid, Z2_valid])\n",
    "        \n",
    "        ZZ = np.float32(np.concatenate(([Z1_train, Z2_train]), axis=1))\n",
    "        DD = np.diag(np.repeat([lam1, lam2],q))\n",
    "        \n",
    "        v_pred = DD@ZZ.T@(tf.eye(N_train) - ZZ@DD@inv(ZZ.T@ZZ@DD+phi*tf.eye(Q))@ZZ.T)@(y_train.reshape(N_train,1)-fX_train)/phi\n",
    "        v1_pred, v2_pred = v_pred[:q], v_pred[q:]\n",
    "        \n",
    "        mu_train = fX_train + Z1_train@v1_pred + Z2_train@v2_pred\n",
    "        mu_valid = fX_valid + Z1_valid@v1_pred + Z2_valid@v2_pred\n",
    "        \n",
    "        train_mse_marginal[repeat, epoch] = (np.sum(np.square(y_train.reshape(N_train,1)-mu_train))/N_train)\n",
    "        valid_mse_marginal[repeat, epoch] = (np.sum(np.square(y_valid.reshape(N_valid,1)-mu_valid))/N_train)\n",
    "        time_marginal[repeat, epoch] = (time.time() - temp_start)\n",
    "    \n",
    "    H = make_mean_model()\n",
    "    phi, lam1, lam2 = phi_init, lam1_init, lam2_init\n",
    "    \n",
    "    patience_hlik = 0\n",
    "    min_hlik_val_loss = np.infty\n",
    "    temp_start = time.time()    \n",
    "    for epoch in range(max_epochs):\n",
    "        \n",
    "        if epoch!=0:  \n",
    "            hlik_train_loss = train_mean_one_epoch(H, hlik_batch, hlik_loss_mean, mean_optimizer, phi, lam1, lam2, batch_ratio)\n",
    "            hlik_val_loss = hlik_loss_mean(H, [X_valid, Z1_valid, Z2_valid], y_valid, phi, lam1, lam2, batch_ratio=(0.2/0.8))\n",
    "            \n",
    "            v1, v2 = H.get_weights()[-2], H.get_weights()[-1]\n",
    "            phi, lam1, lam2 = np.var(y_train-mu_train), np.var(v1), np.var(v2)\n",
    "        \n",
    "        mu_train = np.sum(H([X_train, Z1_train, Z2_train]),axis=0).T\n",
    "        mu_valid = np.sum(H([X_valid, Z1_valid, Z2_valid]),axis=0).T\n",
    "        \n",
    "        train_mse_hlik[repeat, epoch] = (np.sum(np.square(y_train-mu_train))/N_train)\n",
    "        valid_mse_hlik[repeat, epoch] = (np.sum(np.square(y_valid-mu_valid))/N_valid)\n",
    "        time_hlik[repeat, epoch] = (time.time() - temp_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e351b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5), dpi=300)\n",
    "\n",
    "plt.subplot(121)\n",
    "for repeat in range(n_simul):\n",
    "    if repeat==0:\n",
    "        plt.plot(time_marginal[repeat,], train_mse_marginal[repeat,], linestyle='dashed', color='orangered', dashes=(10, 5), linewidth=0.5, label='integrated') \n",
    "        plt.plot(time_hlik[repeat,], train_mse_hlik[repeat,], linestyle='solid', color='royalblue', linewidth=0.3, label='h-likelihood')\n",
    "    else:\n",
    "        plt.plot(time_marginal[repeat,], train_mse_marginal[repeat,], linestyle='dashed', color='orangered', dashes=(10, 5), linewidth=0.5) \n",
    "        plt.plot(time_hlik[repeat,], train_mse_hlik[repeat,], linestyle='solid', color='royalblue', linewidth=0.3)\n",
    "plt.xlim([0, 10])\n",
    "plt.xlabel('Time (sec)', fontsize=12)\n",
    "plt.ylabel('Train MSE', fontsize=12)\n",
    "plt.legend(loc='upper right', ncol=1, fontsize=12)\n",
    "plt.title('Train MSE')\n",
    "\n",
    "plt.subplot(122)\n",
    "for repeat in range(n_simul):\n",
    "    if repeat==0:\n",
    "        plt.plot(time_marginal[repeat,], valid_mse_marginal[repeat,], linestyle='dashed', color='orangered', dashes=(10, 5), linewidth=0.5, label='integrated') \n",
    "        plt.plot(time_hlik[repeat,], valid_mse_hlik[repeat,], linestyle='solid', color='royalblue', linewidth=0.3, label='h-likelihood')\n",
    "    else:\n",
    "        plt.plot(time_marginal[repeat,], valid_mse_marginal[repeat,], linestyle='dashed', color='orangered', dashes=(10, 5), linewidth=0.5) \n",
    "        plt.plot(time_hlik[repeat,], valid_mse_hlik[repeat,], linestyle='solid', color='royalblue', linewidth=0.3) \n",
    "plt.xlim([0, 10])\n",
    "plt.xlabel('Time (sec)', fontsize=12)\n",
    "plt.ylabel('Validation MSE', fontsize=12)\n",
    "plt.legend(loc='upper right', ncol=1, fontsize=12)\n",
    "plt.title('Validation MSE')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
